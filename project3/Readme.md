# ğŸ©º Medical Text Classification Project
This project demonstrates the implementation of different machine learning models (Logistic Regression, Decision Tree, Random Forest, and XGBoost) for **classifying medical transcripts** into four categories.  
The goal is to analyze model performance on training, validation, and test datasets, and to compare two text representations: **Binary Bag-of-Words (BBoW)** and **Frequency Bag-of-Words (FBoW)**.

---

ğŸ”¹ **Project Overview**
- Load datasets (train, valid, test)  
- Preprocess text (lowercasing, punctuation removal, stopword removal, lemmatization)  
- Build vocabulary (top 10,000 most frequent words)  
- Implement vectorization methods:  
  - **BBoW (Binary Bag-of-Words)**  
  - **FBoW (Frequency Bag-of-Words)**  
- Train four models: Logistic Regression, Decision Tree, Random Forest, XGBoost  
- Compute **Macro F1-score** to compare training, validation, and test performance  
- Visualize hyperparameter effects on underfitting and overfitting  

---

ğŸ› ï¸ **Technologies Used**
- Python  
- NumPy  
- Pandas  
- scikit-learn  
- XGBoost  
- NLTK  
- Matplotlib  

---

ğŸ“‚ **Project Structure**
- `Medical_Text_Classification.ipynb` â†’ Jupyter Notebook with implementation  
- `train.csv` â†’ Training dataset  
- `valid.csv` â†’ Validation dataset  
- `test.csv` â†’ Test dataset  
- `vocab.txt` â†’ Vocabulary file (word, ID, frequency)  

---

ğŸ“Š **Results**

ğŸ”¹ **Binary Bag-of-Words (BBoW)**
- Logistic Regression â†’ Train F1 â‰ˆ 0.82, Test F1 â‰ˆ 0.50  
- Decision Tree â†’ Train F1 â‰ˆ 0.82, Test F1 â‰ˆ 0.47  
- Random Forest â†’ Train F1 â‰ˆ 0.82, Test F1 â‰ˆ 0.27  
- XGBoost â†’ Train F1 â‰ˆ 0.82, Test F1 â‰ˆ 0.57  

ğŸ‘‰ **Observation:** All models achieve similar performance on training (~0.82) but drop on test. XGBoost generalizes best, Random Forest performs worst.  

ğŸ”¹ **Frequency Bag-of-Words (FBoW)**
- Expected to perform better because frequency information is preserved.  
- Captures richer semantics compared to BBoW.  

---

ğŸ¯ **Learning Objectives**
- Understand text preprocessing for NLP tasks  
- Learn how to build a vocabulary and vectorize text data (BBoW vs FBoW)  
- Compare classical machine learning models for text classification  
- Analyze underfitting and overfitting via hyperparameter tuning  
- Practice evaluating models using **Macro F1-score**  

---
